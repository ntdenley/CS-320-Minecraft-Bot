{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 modules: array, and nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml.array as ml\n",
    "import ml.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(11.904297, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = ml.Array([1,2,3,4])\n",
    "b = ml.Array([5,6,7,8])\n",
    "\n",
    "out = (a + b).sqrt().sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View computation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.view_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44757596 0.39696687 0.50973177 0.3599454 ]\n",
      " [0.38673344 0.32849854 0.43344077 0.31179613]]\n",
      "[[0.7345767  0.39309776]\n",
      " [0.9321716  0.50864315]\n",
      " [1.1951847  0.64410883]\n",
      " [0.72826785 0.36319292]]\n"
     ]
    }
   ],
   "source": [
    "a = ml.rand([2,4])\n",
    "b = ml.rand([4,2])\n",
    "c = ml.rand([2])\n",
    "out = (a @ b).sqrt().sum()\n",
    "\n",
    "# backward \n",
    "out.build_backward()\n",
    "print(a.grad().eval())\n",
    "print(b.grad().eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.view_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning: Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_data():\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train = ml.Array(X_train.tolist())\n",
    "    X_test = ml.Array(X_test.tolist())\n",
    "    y_train = ml.Array(y_train.tolist())\n",
    "    y_test = ml.Array(y_test.tolist())\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def test_accuracy(W, b):\n",
    "    X_train, X_test, y_train, y_test = get_data()\n",
    "    logits_test = X_test.eval() @ W.eval() + (b.eval())\n",
    "    predictions = np.argmax(logits_test, axis=1)\n",
    "    accuracy = np.mean((predictions == y_test.eval()))\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss =  1.073671817779541\n",
      "10 loss =  0.34376946091651917\n",
      "20 loss =  0.2100478857755661\n",
      "30 loss =  0.15509305894374847\n",
      "40 loss =  0.12708038091659546\n",
      "50 loss =  0.11043570190668106\n",
      "60 loss =  0.10024851560592651\n",
      "70 loss =  0.09293965995311737\n",
      "80 loss =  0.0873216837644577\n",
      "90 loss =  0.08281934261322021\n",
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "def train(lr, iter):\n",
    "    X_train, X_test, y_train, y_test = get_data()\n",
    "    one_hot_train = nn.one_hot_encode(y_train, 3)\n",
    "    \n",
    "    # fully connected layer\n",
    "    W = ml.rand([4,3])\n",
    "    b = ml.rand([3])\n",
    "    logits = X_train @ W + b\n",
    "\n",
    "    # built in loss functions\n",
    "    loss = nn.softmax_cross_entropy_loss(logits, one_hot_train) \n",
    "    loss.build_backward()\n",
    "\n",
    "    # optimizers supported: Adam, SGD\n",
    "    optim = nn.Adam(lr, [W,b])\n",
    "\n",
    "    # training loop\n",
    "    for it in range(iter):\n",
    "        if (it % (iter//10) == 0): print(f\"{it} loss = \", loss.eval().item())\n",
    "        loss.eval()\n",
    "        loss.zero_grad()\n",
    "        loss.set_reeval()\n",
    "        W.grad().set_reeval()\n",
    "        b.grad().set_reeval()\n",
    "        optim.step()\n",
    "    \n",
    "    test_accuracy(W,b)\n",
    "    return loss\n",
    "\n",
    "lr = 0.01\n",
    "iter = 100\n",
    "\n",
    "loss = train(lr,iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view neural network graph\n",
    "loss.view_graph(view_data=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
